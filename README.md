# Mini HPC Cluster Simulation

<p align="center">
  <strong>A Docker-based High-Performance Computing environment for learning distributed computing, fault tolerance, and cluster monitoring.</strong>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Docker-Ready-blue" alt="Docker">
  <img src="https://img.shields.io/badge/Python-3.x-green" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-Distributed-red" alt="PyTorch">
  <img src="https://img.shields.io/badge/License-MIT-yellow" alt="License">
</p>

---

## ğŸ¯ Overview

Mini HPC simulates a production-grade HPC cluster with automatic fault recovery, real-time monitoring, and distributed job execution. Perfect for learning distributed systems, container orchestration, and observability patterns.

### Key Features

| Feature | Description |
|---------|-------------|
| ğŸ—ï¸ **Distributed Architecture** | 1 master + 2 worker nodes with Docker networking |
| ğŸ”„ **Fault Tolerance** | Auto-restart policies, health checks, recovery monitoring |
| ğŸ“Š **Observability** | Prometheus metrics + Grafana dashboards |
| ğŸš€ **Job Management** | Queue system with logging and distributed execution |
| ğŸ¤– **ML Support** | Distributed PyTorch training with Gloo backend |
| âš¡ **Easy Operations** | Makefile commands for all cluster operations |

---

## ğŸš€ Quick Start

```bash
# Clone and setup
git clone https://github.com/faraz-irshad/mini-hpc-cluster.git
cd mini-hpc-cluster

# Install dependencies
pip install -r requirements.txt

# Build and start cluster
make build
make up

# Submit your first job
make submit

# Check cluster health
make health
```

**Access dashboards:**
- Prometheus: http://localhost:9090
- Grafana: http://localhost:3000 (admin/admin)

---

## ğŸ“‹ Prerequisites

- **Docker** (v20.10+) & **Docker Compose** (v2.0+)
- **Python 3.8+**
- **Linux/macOS** (tested on Pop!_OS/Ubuntu)
- **4GB RAM** minimum (8GB recommended)

---

## ğŸ® Command Reference

### Cluster Operations

```bash
make build          # Build Docker images
make up             # Start cluster (detached mode)
make down           # Stop cluster
make status         # Show container status
make clean          # Remove containers, volumes, and logs
```

### Job Management

```bash
make submit         # Submit jobs from jobs/ directory
make logs           # View job execution logs
make monitor        # Real-time job monitoring (updates every 2s)
```

### Fault Tolerance

```bash
make health         # Check cluster health status
make test-failure   # Simulate node failure (interactive)
make watch-recovery # Monitor recovery in real-time (updates every 5s)
```

### Advanced

```bash
# Manual job submission
python3 submit_job.py

# Direct failure simulation
python3 simulate_failure.py worker-node-1

# Health check
python3 recovery_monitor.py

# Restart strategies
make down && make up                    # Quick restart
make clean && make build && make up     # Full rebuild
```

---

## ğŸ“ Project Structure

```
mini-hpc/
â”œâ”€â”€ ğŸ“‚ jobs/                          # Job scripts directory
â”‚   â”œâ”€â”€ hello.py                     # Simple test job
â”‚   â”œâ”€â”€ benchmark.py                 # Performance benchmark
â”‚   â””â”€â”€ train_mnist_distributed.py   # Distributed PyTorch training
â”‚
â”œâ”€â”€ ğŸ“‚ logs/                          # Job execution logs (auto-generated)
â”œâ”€â”€ ğŸ“‚ screenshots/                   # Dashboard screenshots
â”‚
â”œâ”€â”€ ğŸ³ Dockerfile                     # Container image definition
â”œâ”€â”€ ğŸ³ docker-compose.yml             # Cluster orchestration
â”‚
â”œâ”€â”€ ğŸ“Š prometheus.yml                 # Metrics collection config
â”œâ”€â”€ ğŸ”§ metrics_exporter.py            # Custom metrics exporter
â”‚
â”œâ”€â”€ ğŸš€ submit_job.py                  # Job submission interface
â”œâ”€â”€ ğŸ“‹ job_queue.py                   # Job queue management
â”œâ”€â”€ ğŸ‘ï¸ monitor_jobs.py                # Job monitoring tool
â”‚
â”œâ”€â”€ ğŸ”„ simulate_failure.py            # Fault tolerance testing
â”œâ”€â”€ ğŸ’š recovery_monitor.py            # Health & recovery monitoring
â”‚
â”œâ”€â”€ ğŸ› ï¸ setup_grafana.sh               # Grafana dashboard setup
â”œâ”€â”€ âš™ï¸ Makefile                       # Command shortcuts
â”œâ”€â”€ ğŸ“¦ requirements.txt               # Python dependencies
â”‚
â”œâ”€â”€ ğŸ“– README.md                      # This file
â”œâ”€â”€ ğŸ“– FAULT_TOLERANCE.md             # Fault tolerance guide
â””â”€â”€ ğŸ“– METRICS_GUIDE.md               # Metrics documentation
```

---

## ğŸ’¼ Working with Jobs

### Creating Jobs

1. Create a Python script in the `jobs/` directory:

```python
# jobs/my_custom_job.py
import os
import time

node = os.uname().nodename
rank = os.environ.get("RANK", "0")

print(f"[Rank {rank}] Starting job on {node}")
time.sleep(2)
print(f"[Rank {rank}] Job completed successfully!")
```

2. Submit the job:

```bash
make submit
```

3. View logs:

```bash
make logs                              # Interactive monitoring
cat logs/worker-node-1_my_custom_job.log  # Direct log access
```

### Example Jobs

**Simple Hello World** (`hello.py`)
```python
import os
rank = os.environ.get("RANK", "0")
node = os.uname().nodename
print(f"[Rank {rank}] Hello from {node}!")
```

**Performance Benchmark** (`benchmark.py`)
```bash
make submit  # Runs CPU/memory benchmarks across all nodes
```

**Distributed ML Training** (`train_mnist_distributed.py`)
```bash
make submit  # Distributed PyTorch training with Gloo backend
```

---

## ğŸ“Š Monitoring & Observability

### Dashboards

| Service | URL | Credentials |
|---------|-----|-------------|
| **Prometheus** | http://localhost:9090 | None |
| **Grafana** | http://localhost:3000 | admin/admin |

### Available Metrics

```promql
# CPU usage per node
node_cpu_percent

# Memory usage percentage  
node_memory_percent

# Memory used (GB)
node_memory_MemUsed_bytes / 1024 / 1024 / 1024

# Disk I/O
node_disk_io_time_seconds_total
```

### Live Dashboard

![HPC Cluster Monitoring](screenshots/grafana-dashboard.png)

*Real-time CPU and memory metrics across all nodes during distributed job execution.*

### Log Files

- **Job logs:** `logs/<node>_<job>.log`
- **Cluster log:** `hpc_cluster.log`
- **Container logs:** `docker logs <container-name>`

---

## ğŸ”„ Fault Tolerance & Recovery

### Automatic Recovery Features

- **Auto-restart policies:** Containers restart automatically on failure
- **Health checks:** 30-second intervals with 3 retries
- **Recovery monitoring:** Real-time status tracking
- **Graceful degradation:** Cluster continues with available nodes

### Testing Fault Tolerance

```bash
# Check current cluster health
make health

# Simulate node failure (interactive)
make test-failure
# Choose: master-node, worker-node-1, or worker-node-2

# Direct failure simulation
python3 simulate_failure.py worker-node-1

# Watch recovery process
make watch-recovery
```

### Recovery Process

1. **Failure detected** â†’ Health check fails or container exits
2. **Auto-restart triggered** â†’ Docker restarts container
3. **Health checks resume** â†’ 10s grace period, then monitoring
4. **Node rejoins cluster** â†’ Ready for new jobs

**Expected recovery time:** 15-30 seconds

ğŸ“– **Detailed guide:** [FAULT_TOLERANCE.md](FAULT_TOLERANCE.md)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Docker Network (hpcnet)              â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Master Node  â”‚  â”‚ Worker Node 1â”‚  â”‚ Worker Node 2â”‚ â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚ â”‚
â”‚  â”‚ Job Queue    â”‚  â”‚ Job Executor â”‚  â”‚ Job Executor â”‚ â”‚
â”‚  â”‚ Coordinator  â”‚  â”‚ Metrics      â”‚  â”‚ Metrics      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                 â”‚                 â”‚          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                           â”‚                            â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚         â”‚                                   â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Prometheus   â”‚                 â”‚    Grafana    â”‚  â”‚
â”‚  â”‚  :9090        â”‚                 â”‚    :3000      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§ª Testing

```bash
# 1. Start cluster
make up

# 2. Verify all nodes are healthy
make health

# 3. Submit test job
make submit

# 4. Monitor execution
make monitor

# 5. Test fault tolerance
make test-failure

# 6. Verify recovery
make health
```

---

## ğŸ› Troubleshooting

### Cluster won't start
```bash
# Check Docker status
docker compose ps

# View logs
docker compose logs

# Clean and rebuild
make clean && make build && make up
```

### Node not recovering
```bash
# Check restart count
docker inspect worker-node-1 --format='{{.RestartCount}}'

# View container logs
docker logs worker-node-1 --tail 50

# Manual restart
docker restart worker-node-1
```

### Jobs not executing
```bash
# Check job queue
cat job_queue.json

# Verify node connectivity
docker exec master-node ping worker1

# Check logs
make logs
```

---

## ğŸ“š Documentation

- **[FAULT_TOLERANCE.md](FAULT_TOLERANCE.md)** - Fault tolerance guide and testing scenarios
- **[METRICS_GUIDE.md](METRICS_GUIDE.md)** - Metrics collection and Prometheus queries

---

## ğŸ“ Learning Outcomes

This project demonstrates:

- âœ… Container orchestration with Docker Compose
- âœ… Distributed system design patterns
- âœ… Fault tolerance and auto-recovery mechanisms
- âœ… Observability with Prometheus and Grafana
- âœ… Job scheduling and queue management
- âœ… Distributed ML training with PyTorch
- âœ… Infrastructure as Code (IaC) practices

---

## âš ï¸ Limitations

- **Educational purpose only** - Not production-ready
- **CPU-only** - No GPU support (lightweight)
- **No job persistence** - Jobs lost on node failure
- **Manual resubmission** - No automatic job retry
- **Single-host** - All containers on one machine

---

## ğŸš€ Future Enhancements

- [ ] Job checkpointing and auto-resume
- [ ] GPU support for ML workloads
- [ ] Kubernetes deployment option
- [ ] REST API for job submission
- [ ] Web UI dashboard
- [ ] Multi-host cluster support
- [ ] Advanced scheduling algorithms

---

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit issues and pull requests.

---

## ğŸ“„ License

MIT License - see LICENSE file for details

---

## ğŸ‘¤ Author

**Faraz Irshad**
- GitHub: [@faraz-irshad](https://github.com/faraz-irshad)

---

<p align="center">Made with â¤ï¸ for learning distributed systems</p>
